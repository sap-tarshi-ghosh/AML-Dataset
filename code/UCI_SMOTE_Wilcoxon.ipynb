{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6ASlF/VNOtQpxGafGz0+C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sap-tarshi-ghosh/AML-Dataset/blob/main/code/UCI_SMOTE_Wilcoxon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hov5szk06wuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc041e1b-aecd-4ae4-87f5-5acf5d66b8b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wilcoxon statistic = 5.0000\n",
            "p-value = 1.0000\n",
            "Effect size (r) = 0.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import wilcoxon\n",
        "\n",
        "# Load Excel file\n",
        "file_path = \"UCI_Dataset_SMOTE_ALL.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract only 10-fold CV data (rows 0â€“9) and ensure it's a copy to avoid SettingWithCopyWarning\n",
        "cv_data = df.iloc[0:10].copy()\n",
        "\n",
        "# Convert model columns to numeric, coercing errors to NaN\n",
        "cv_data[\"AdaBoost\"] = pd.to_numeric(cv_data[\"AdaBoost\"], errors='coerce')\n",
        "cv_data[\"Bagging\"] = pd.to_numeric(cv_data[\"Bagging\"], errors='coerce')\n",
        "\n",
        "# Select the two models to compare\n",
        "model_A = cv_data[\"AdaBoost\"]\n",
        "model_B = cv_data[\"Bagging\"]\n",
        "\n",
        "# Compute differences\n",
        "diff = model_A - model_B\n",
        "\n",
        "# Remove zero differences (Wilcoxon requirement) and NaN values\n",
        "non_zero_diff = diff[diff != 0].dropna()\n",
        "\n",
        "# Perform Wilcoxon signed-rank test (two-sided)\n",
        "stat, p_value = wilcoxon(\n",
        "    non_zero_diff,\n",
        "    alternative=\"two-sided\",\n",
        "    zero_method=\"wilcox\",\n",
        "    mode=\"auto\"\n",
        ")\n",
        "\n",
        "# Effect size calculation\n",
        "# According to https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test#Effect_size\n",
        "# This calculation needs to handle the case where non_zero_diff is empty to prevent division by zero or sqrt(negative).\n",
        "# If non_zero_diff is empty, stat would be 0, and len(non_zero_diff) would be 0.\n",
        "if len(non_zero_diff) > 0:\n",
        "    # Recalculate z based on the formula for effect size, not the stat value itself\n",
        "    # The previous `z = stat - (len(non_zero_diff) * (len(non_zero_diff) + 1)) / 4` is specific to the test statistic, not for effect size calculation 'r'\n",
        "    # For effect size r, we usually need the W statistic and N (number of non-zero differences)\n",
        "    # The provided formula in the comment links to a different interpretation of Z/r\n",
        "    # A common effect size for Wilcoxon is r = Z / sqrt(N)\n",
        "    # The 'z' variable in the original code is not directly the 'Z' from normal approximation for r\n",
        "\n",
        "    # Let's use a common approach where Z is derived from the test statistic for large N\n",
        "    # For small N, directly computing 'r' is more complex or uses different approximations\n",
        "    # Given the previous value for stat was 5.0 and non_zero_diff has 4 elements:\n",
        "\n",
        "    # The previous calculation for 'z' directly used 'stat' in a way that might not align with 'r = Z / sqrt(N)'\n",
        "    # Let's re-evaluate the effect size calculation provided in the original code snippet\n",
        "    # The formula used for z and effect_size_r was:\n",
        "    # z = stat - (len(non_zero_diff) * (len(non_zero_diff) + 1)) / 4\n",
        "    # z /= np.sqrt(len(non_zero_diff) * (len(non_zero_diff) + 1) * (2 * len(non_zero_diff) + 1) / 24)\n",
        "    # effect_size_r = abs(z) / np.sqrt(len(non_zero_diff))\n",
        "\n",
        "    # This 'z' is essentially a standardized test statistic. Let's stick to the user's provided formula if it's what they intended.\n",
        "    # The issue might be that with N=4, the normal approximation for Z is not very accurate, leading to odd r values.\n",
        "    # However, if non_zero_diff is empty, the calculation would fail.\n",
        "\n",
        "    # Re-using the existing effect size calculation method, ensuring N > 0\n",
        "    rank_sum_expected = len(non_zero_diff) * (len(non_zero_diff) + 1) / 4\n",
        "    std_dev_rank_sum = np.sqrt(len(non_zero_diff) * (len(non_zero_diff) + 1) * (2 * len(non_zero_diff) + 1) / 24)\n",
        "\n",
        "    if std_dev_rank_sum > 0:\n",
        "        z_score_for_r = (stat - rank_sum_expected) / std_dev_rank_sum\n",
        "        effect_size_r = abs(z_score_for_r) / np.sqrt(len(non_zero_diff))\n",
        "    else:\n",
        "        effect_size_r = 0.0 # If std_dev is 0, all differences might be the same, or N is too small\n",
        "else:\n",
        "    effect_size_r = 0.0 # If there are no non-zero differences, effect size is 0\n",
        "\n",
        "print(f\"Wilcoxon statistic = {stat:.4f}\")\n",
        "print(f\"p-value = {p_value:.4f}\")\n",
        "print(f\"Effect size (r) = {effect_size_r:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import wilcoxon\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "# ==========================================\n",
        "# FILE PATH\n",
        "# ==========================================\n",
        "\n",
        "file_path = \"UCI_Dataset_SMOTE_ALL.xlsx\"\n",
        "\n",
        "# ==========================================\n",
        "# METRIC DIRECTION (FULLY DEFINED)\n",
        "# True  -> higher is better\n",
        "# False -> lower is better\n",
        "# ==========================================\n",
        "\n",
        "metric_direction = {\n",
        "    \"Accuracy\": True,\n",
        "    \"Precision\": True,\n",
        "    \"Recall\": True,\n",
        "    \"Sensitivity\": True,\n",
        "    \"Specificity\": True,\n",
        "    \"F1\": True,\n",
        "    \"F1-Score\": True,\n",
        "    \"Kappa\": True,\n",
        "    \"AUC\": True,\n",
        "    \"G-Mean\": True,\n",
        "    \"MCC\": True,\n",
        "    \"NPV\": True,\n",
        "    \"PPV\": True,\n",
        "\n",
        "    \"FPR\": False,\n",
        "    \"FNR\": False,\n",
        "    \"Error\": False,\n",
        "    \"Error Rate\": False,\n",
        "    \"LogLoss\": False,\n",
        "    \"Hamming Loss\": False\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# LOAD EXCEL FILE\n",
        "# ==========================================\n",
        "\n",
        "xls = pd.ExcelFile(file_path)\n",
        "all_results = []\n",
        "\n",
        "# ==========================================\n",
        "# MAIN LOOP OVER ALL METRIC SHEETS\n",
        "# ==========================================\n",
        "\n",
        "for sheet in xls.sheet_names:\n",
        "\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Processing metric: {sheet}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    df = pd.read_excel(xls, sheet_name=sheet)\n",
        "\n",
        "    # Use only first 10 rows (10-fold CV) and create a copy\n",
        "    cv_data = df.iloc[0:10].copy()\n",
        "\n",
        "    # Check if enough rows are present for 10-fold CV\n",
        "    if cv_data.shape[0] != 10:\n",
        "        print(f\"Warning: {sheet}: Expected 10 CV rows, found {cv_data.shape[0]}. Skipping this sheet.\")\n",
        "        continue\n",
        "\n",
        "    # Identify model performance columns by excluding 'Fold' (assuming 'Fold' is the identifier)\n",
        "    # You might need to adjust this list if there are other non-model columns\n",
        "    non_model_columns = ['Fold']\n",
        "    model_performance_cols = [col for col in cv_data.columns if col not in non_model_columns]\n",
        "\n",
        "    # Convert all identified model performance columns to numeric, coercing errors to NaN\n",
        "    for col in model_performance_cols:\n",
        "        cv_data[col] = pd.to_numeric(cv_data[col], errors='coerce')\n",
        "\n",
        "    # Filter to only include model performance columns that are entirely numeric (not all NaN)\n",
        "    cv_data_numeric_models = cv_data[model_performance_cols].dropna(axis=1, how='all')\n",
        "\n",
        "    if cv_data_numeric_models.empty:\n",
        "        print(f\"Warning: No valid numeric model performance data found for sheet '{sheet}'. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Determine direction (default: higher is better)\n",
        "    higher_is_better = metric_direction.get(sheet, True)\n",
        "\n",
        "    # ==========================================\n",
        "    # REFERENCE MODEL SELECTION (MEDIAN-BASED)\n",
        "    # ==========================================\n",
        "\n",
        "    medians = cv_data_numeric_models.median()\n",
        "\n",
        "    if medians.empty:\n",
        "        print(f\"Warning: No medians could be calculated for sheet '{sheet}'. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    if higher_is_better:\n",
        "        reference_model = medians.idxmax()\n",
        "    else:\n",
        "        reference_model = medians.idxmin()\n",
        "\n",
        "    # Ensure ref_values are from the numeric model data\n",
        "    ref_values = cv_data_numeric_models[reference_model]\n",
        "\n",
        "    print(f\"Reference model selected: {reference_model}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # WILCOXON SIGNED-RANK TESTS\n",
        "    # ==========================================\n",
        "\n",
        "    compared_models = []\n",
        "    wilcoxon_stats = []\n",
        "    raw_pvals = []\n",
        "\n",
        "    for model in cv_data_numeric_models.columns: # Iterate only over valid numeric model columns\n",
        "        if model == reference_model:\n",
        "            continue\n",
        "\n",
        "        current_model_values = cv_data_numeric_models[model]\n",
        "\n",
        "        # Create a temporary DataFrame to handle NaNs and align pairs for comparison\n",
        "        temp_df = pd.DataFrame({'ref': ref_values, 'comp': current_model_values}).dropna()\n",
        "\n",
        "        # If not enough valid pairs after dropping NaNs, mark as unreliable\n",
        "        if len(temp_df) < 5: # Consider 5 as a minimum number of valid pairs for reliable test\n",
        "            stat = np.nan\n",
        "            p_val = np.nan\n",
        "        else:\n",
        "            # Perform Wilcoxon signed-rank test on the paired, non-NaN values\n",
        "            stat, p_val = wilcoxon(\n",
        "                temp_df['ref'].values,\n",
        "                temp_df['comp'].values,\n",
        "                zero_method=\"wilcox\",\n",
        "                alternative=\"two-sided\"\n",
        "            )\n",
        "\n",
        "        compared_models.append(model)\n",
        "        wilcoxon_stats.append(stat)\n",
        "        raw_pvals.append(p_val)\n",
        "\n",
        "    # ==========================================\n",
        "    # MULTIPLE COMPARISON CORRECTION (HOLM)\n",
        "    # ==========================================\n",
        "\n",
        "    # Filter out NaN p-values for multipletests and then re-assign\n",
        "    valid_pvals_indices = [i for i, p in enumerate(raw_pvals) if not pd.isna(p)]\n",
        "\n",
        "    if valid_pvals_indices:\n",
        "        valid_pvals = [raw_pvals[i] for i in valid_pvals_indices]\n",
        "        reject_valid, p_adj_valid, _, _ = multipletests(\n",
        "            valid_pvals,\n",
        "            method=\"holm\",\n",
        "            alpha=0.05\n",
        "        )\n",
        "\n",
        "        # Reconstruct p_adj and reject lists, filling NaNs where original raw_pvals were NaN\n",
        "        p_adj = [np.nan] * len(raw_pvals)\n",
        "        reject = [False] * len(raw_pvals)\n",
        "        for i, original_idx in enumerate(valid_pvals_indices):\n",
        "            p_adj[original_idx] = p_adj_valid[i]\n",
        "            reject[original_idx] = reject_valid[i]\n",
        "    else:\n",
        "        # No valid p-values to correct\n",
        "        p_adj = [np.nan] * len(raw_pvals)\n",
        "        reject = [False] * len(raw_pvals)\n",
        "\n",
        "    result_df = pd.DataFrame({\n",
        "        \"Metric\": sheet,\n",
        "        \"Reference_Model\": reference_model,\n",
        "        \"Compared_Model\": compared_models,\n",
        "        \"Wilcoxon_Statistic\": wilcoxon_stats,\n",
        "        \"Raw_p_value\": raw_pvals,\n",
        "        \"Adjusted_p_value\": p_adj,\n",
        "        \"Significant_after_Holm\": reject\n",
        "    })\n",
        "\n",
        "    all_results.append(result_df)\n",
        "\n",
        "# ==========================================\n",
        "# FINAL RESULT TABLE\n",
        "# ==========================================\n",
        "\n",
        "if all_results:\n",
        "    final_results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "    final_results.to_excel(\n",
        "        \"Wilcoxon_Reference_Based_Results.xlsx\",\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    print(\"\\n========================================\")\n",
        "    print(\"Analysis complete.\")\n",
        "    print(\"Results saved to: Wilcoxon_Reference_Based_Results.xlsx\")\n",
        "    print(\"========================================\")\n",
        "else:\n",
        "    print(\"\\n========================================\")\n",
        "    print(\"Analysis complete, but no valid results to save.\")\n",
        "    print(\"========================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrM6HaB3UZzk",
        "outputId": "5160d02a-b5d9-44cd-d85b-c1d4de186f33"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Processing metric: Accuracy\n",
            "==============================\n",
            "Reference model selected: Random Forest\n",
            "\n",
            "==============================\n",
            "Processing metric: AUC\n",
            "==============================\n",
            "Reference model selected: XGBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: F1-Score\n",
            "==============================\n",
            "Reference model selected: XGBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: FPR\n",
            "==============================\n",
            "Reference model selected: AdaBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: GM\n",
            "==============================\n",
            "Reference model selected: XGBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: Kappa\n",
            "==============================\n",
            "Reference model selected: XGBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: MCC\n",
            "==============================\n",
            "Reference model selected: XGBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: Precision\n",
            "==============================\n",
            "Reference model selected: AdaBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: Recall\n",
            "==============================\n",
            "Reference model selected: KNN\n",
            "\n",
            "==============================\n",
            "Processing metric: Specificity\n",
            "==============================\n",
            "Reference model selected: AdaBoost\n",
            "\n",
            "==============================\n",
            "Processing metric: Sheet1\n",
            "==============================\n",
            "Warning: Sheet1: Expected 10 CV rows, found 0. Skipping this sheet.\n",
            "\n",
            "========================================\n",
            "Analysis complete.\n",
            "Results saved to: Wilcoxon_Reference_Based_Results.xlsx\n",
            "========================================\n"
          ]
        }
      ]
    }
  ]
}